{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running trained BCA model for predicting text complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads the pre-trained BCA model, and runs it on a test set; there is also the option to visualize attention at the word and senetce level. \n",
    "The model has been train to map grades K-12 to 12 levels:\n",
    "\n",
    "K-1 -> 0\n",
    "\n",
    "2-12 -> 1-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained model can be obtained from https://1drv.ms/f/s!Ag4UUgKkf0ZPu55vBR6_-6WOuAO-Ug. A test set is also available at https://sites.google.com/site/nadeemf0755/research/linguistic-complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import io\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = 10000)\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell as Cell #for GRU: custom implementation with normalization\n",
    "from tensorflow.python.ops.rnn import dynamic_rnn as rnn\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tensorflow.contrib.rnn import DropoutWrapper\n",
    "\n",
    "from attention import attention as attention\n",
    "from bca import *\n",
    "from ordloss import *\n",
    "from utils import *\n",
    "#from dataUtils_snli import *\n",
    "from dataUtils_gr import *\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionary from the pre-trained model folder\n",
    "import csv \n",
    "dictionary = {}\n",
    "for key,val in csv.reader(open('BCA_grades/dict_bca_gr250.csv')):\n",
    "    dictionary[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the test data set; the fformat is csv, with the text column labelled 'text'\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>who are you ? the lion yelled at my father .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>my name is elmer elevator .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>where do you think you are going ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>i 'm going home , said my father .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>that 's what you think ! said the lion . ordin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                               text\n",
       "0           0    2.0      who are you ? the lion yelled at my father . \n",
       "1           1    2.0                       my name is elmer elevator . \n",
       "2           2    2.0                where do you think you are going ? \n",
       "3           3    2.0                i 'm going home , said my father . \n",
       "4           4    2.0  that 's what you think ! said the lion . ordin..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform basic text clean-up, lower casing, and convert British spelling to US \n",
    "text = []\n",
    "for i in range(len(df_test)):\n",
    "    t = df_test.iloc[i]['text']\n",
    "    text.append(clean_(t))\n",
    "df_test['text'] = text\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 40\n",
    "SEQUENCE_LENGTH_D = 25\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of test set:  3830\n",
      "IV in test set:  87624\n",
      "OOV in test set:  2057\n"
     ]
    }
   ],
   "source": [
    "X_test_ = read_test_set(df_test, dictionary, SEQUENCE_LEN_D = SEQUENCE_LENGTH_D, SEQUENCE_LEN = SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(dictionary)\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reload the trained model; get reading level predictions for test set \n",
    "\"\"\"\n",
    "\n",
    "NUM_WORDS = vocabulary_size\n",
    "INDEX_FROM = 3\n",
    "#EMBEDDING_DIM = embedding_dim\n",
    "# system parameters\n",
    "HIDDEN_SIZE = 150\n",
    "HIDDEN_SIZE_D = 100\n",
    "ATTENTION_SIZE = 75\n",
    "ATTENTION_SIZE_D = 50\n",
    "#PROJECTION_SIZE = 100\n",
    "KEEP_PROB = 0.5\n",
    "NUM_EPOCHS = 5  # max val_acc at __\n",
    "DELTA = 0.75\n",
    "ordinal = True\n",
    "\n",
    "\n",
    "#Different placeholders\n",
    "num_classes = 12\n",
    "batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH])\n",
    "ind_list_ph = tf.placeholder(tf.int32, [None])\n",
    "target_ph = tf.placeholder(tf.float32, [None,num_classes])\n",
    "seq_len_ph = tf.placeholder(tf.int32, [None])\n",
    "seq_len_ph_d = tf.placeholder(tf.int32, [None])\n",
    "keep_prob_ph = tf.placeholder(tf.float32)\n",
    "doc_size_ph = tf.placeholder(tf.int32,[None])\n",
    "# Embedding layer\n",
    "embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)\n",
    "batch_embedded = tf.nn.dropout(batch_embedded, keep_prob_ph)\n",
    "\n",
    "W_omega = tf.Variable(tf.random_uniform([HIDDEN_SIZE*2, HIDDEN_SIZE*2], -1.0, 1.0))\n",
    "# (Bi-)RNN layer(-s)\n",
    "with tf.variable_scope('sentence'):\n",
    "    fw_cell = Cell(HIDDEN_SIZE)\n",
    "    bw_cell = Cell(HIDDEN_SIZE)\n",
    "    \n",
    "    fw_cell = DropoutWrapper(fw_cell, input_keep_prob=keep_prob_ph, \n",
    "                             output_keep_prob=keep_prob_ph,state_keep_prob=keep_prob_ph,\n",
    "                             variational_recurrent=True, input_size=batch_embedded.get_shape()[-1], \n",
    "                             dtype = tf.float32)\n",
    "    bw_cell = DropoutWrapper(bw_cell, input_keep_prob=keep_prob_ph, \n",
    "                             output_keep_prob=keep_prob_ph,state_keep_prob= keep_prob_ph,\n",
    "                             variational_recurrent=True, input_size=batch_embedded.get_shape()[-1], \n",
    "                             dtype = tf.float32)\n",
    "    rnn_output, _ = bi_rnn(fw_cell, bw_cell, inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "\n",
    "    rnn_outputs_ = cross_attention(rnn_output, SEQUENCE_LENGTH_D, seq_len_ph, BATCH_SIZE, W_omega)\n",
    "    attention_output_, alphas_ = attention(rnn_outputs_ , ATTENTION_SIZE, seq_len_ph, return_alphas = True)\n",
    "    attention_output_ = tf.reshape(attention_output_,[BATCH_SIZE, -1, HIDDEN_SIZE*2*3])\n",
    "    \n",
    "with tf.variable_scope('document'):\n",
    "    fw_cell_d = Cell(HIDDEN_SIZE_D)\n",
    "    bw_cell_d = Cell(HIDDEN_SIZE_D)\n",
    "    \n",
    "    fw_cell_d = DropoutWrapper(fw_cell_d, input_keep_prob=keep_prob_ph, \n",
    "                             output_keep_prob=keep_prob_ph,state_keep_prob=keep_prob_ph,\n",
    "                             variational_recurrent=True, input_size=attention_output_.get_shape()[-1], \n",
    "                             dtype = tf.float32)\n",
    "    bw_cell_d = DropoutWrapper(bw_cell_d, input_keep_prob=keep_prob_ph, \n",
    "                             output_keep_prob=keep_prob_ph,state_keep_prob= keep_prob_ph,\n",
    "                             variational_recurrent=True, input_size=attention_output_.get_shape()[-1], \n",
    "                             dtype = tf.float32)\n",
    "    rnn_outputs_d, _ = bi_rnn(fw_cell_d, bw_cell_d, inputs=attention_output_, \n",
    "                              sequence_length=seq_len_ph_d, dtype=tf.float32)\n",
    "    \n",
    "    #rnn_outputs_d, _ = bi_rnn(Cell(HIDDEN_SIZE_D), Cell(HIDDEN_SIZE_D), inputs=attention_output, sequence_length=seq_len_ph_d, dtype=tf.float32)\n",
    "    attention_output_d, alphas_d = attention(rnn_outputs_d, ATTENTION_SIZE_D, seq_len_ph_d, return_alphas=True)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output_d, keep_prob_ph)\n",
    "\n",
    "\n",
    "if ordinal:\n",
    "    # For ordinal regression, same weights for each class\n",
    "    W = tf.Variable(tf.truncated_normal([drop.get_shape()[1].value], stddev=0.1))\n",
    "    W_ = tf.transpose(tf.reshape(tf.tile(W,[num_classes - 1]),[num_classes - 1, drop.get_shape()[1].value]))\n",
    "    b = tf.Variable(tf.cast(tf.range(num_classes - 1), dtype = tf.float32))\n",
    "    y_hat_ = tf.nn.xw_plus_b(drop, tf.negative(W_), b)\n",
    "\n",
    "    # Predicted labels and logits\n",
    "    y_preds, logits = preds(y_hat_,BATCH_SIZE)\n",
    "    y_true = tf.argmax(target_ph, axis = 1)\n",
    "\n",
    "    # Ordinal loss\n",
    "    loss = ordloss_m(y_hat_, target_ph, BATCH_SIZE)\n",
    "    c = stats.spearmanr\n",
    "    str_score = \"Spearman rank:\"\n",
    "\n",
    "else:\n",
    "    W = tf.Variable(tf.truncated_normal([drop.get_shape()[1].value, num_classes], stddev=0.1))  \n",
    "    b = tf.Variable(tf.truncated_normal([num_classes]))\n",
    "    y_hat_ = tf.nn.xw_plus_b(drop, W, b)\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    y_preds = tf.argmax(y_hat_, axis = 1)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat_, labels=target_ph))\n",
    "    c = accuracy_score\n",
    "    str_score = \"accucary:\"\n",
    "    \n",
    "\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from BCA_grades/model250-21720\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"BCA_grades/model250-21720\"\n",
    "\n",
    "test_batch_generator_ = test_batch_generator(X_test_, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D)\n",
    "\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        #testing\n",
    "        num_batches = X_test_.shape[0] // (BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
    "        a = []\n",
    "        a_d = []\n",
    "        true = []\n",
    "        preds_ = []\n",
    "        log_ = []\n",
    "        for bx in range(num_batches+1):\n",
    "            x_batch = next(test_batch_generator_)\n",
    "            seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "            seq_len_d = []               \n",
    "            l = SEQUENCE_LENGTH_D\n",
    "            for i in range(0,len(x_batch),l):\n",
    "                for j in range(i,i+l):\n",
    "                    if list(x_batch[j]).index(0) == 0:\n",
    "                        seq_len_d.append(j%l)\n",
    "                        break\n",
    "                    elif j == i+l-1:\n",
    "                        seq_len_d.append(l)\n",
    "\n",
    "            seq_len_d = np.array(seq_len_d)\n",
    "\n",
    "            log, pred, alph, alph_d = sess.run([logits, y_preds, alphas_, alphas_d],\n",
    "                         feed_dict={batch_ph: x_batch,\n",
    "                                    seq_len_ph: seq_len,\n",
    "                                    seq_len_ph_d: seq_len_d,\n",
    "                                    keep_prob_ph: 1.0})\n",
    "            a.append(alph)\n",
    "            a_d.append(alph_d)\n",
    "            log_.append(log)\n",
    "            \n",
    "            preds_.extend(pred)\n",
    "   \n",
    "        preds_ = np.array(preds_)\n",
    "        preds_ = preds_.flatten()\n",
    "\n",
    "            \n",
    "        break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display word and sentence level attention\n",
    "from IPython.core.display import display, HTML\n",
    "#Display sentence level attention \n",
    "sc = 0\n",
    "HTML_str_all_sent = []\n",
    "for i in df_test['text']:\n",
    "    i = sent_tokenize(i)\n",
    "    i.insert(0,'START_SENT')\n",
    "    i.append('END_SENT')\n",
    "    bc = 0 \n",
    "    HTML_str = ''\n",
    "    s_len = min(len(i),SEQUENCE_LENGTH_D)\n",
    "\n",
    "    for j in i[:s_len]:\n",
    "        if j != 'START_SENT' and j != 'END_SENT':\n",
    "            j = 'START ' + str(j) + ' END'\n",
    "        x = j.split()\n",
    "        att = a[sc//BATCH_SIZE][((sc%BATCH_SIZE)*SEQUENCE_LENGTH_D)+bc]\n",
    "        att = att/att.max()\n",
    "        bc+=1\n",
    "    \n",
    "        att_sent = []\n",
    "        count = 0\n",
    "\n",
    "        for w in x:\n",
    "            #if w == 'START' or w == 'END':\n",
    "            #    continue\n",
    "            HTML_str = HTML_str + '<font style=\"background: rgba(153, 204, 255, %f)\">%s</font>' % (att[count], w+' ')\n",
    "                \n",
    "            count+=1\n",
    "            if count == SEQUENCE_LENGTH:\n",
    "                break\n",
    "    sc+=1        \n",
    "    HTML_str_all_sent.append(HTML_str)\n",
    "    HTML_str = ''\n",
    "    \n",
    "\n",
    "    \n",
    "#Display sentence level attention \n",
    "sc = 0\n",
    "HTML_str_all_doc = []\n",
    "for i in df_test['text'][:-10]:\n",
    "    i = sent_tokenize(i)\n",
    "    i.insert(0,'START_SENT')\n",
    "    i.append('END_SENT')\n",
    "        \n",
    "    HTML_str = ''\n",
    "    s_len = min(len(i),SEQUENCE_LENGTH_D)\n",
    "    i = i[:s_len]\n",
    "    att = a_d[sc//BATCH_SIZE][sc%BATCH_SIZE][:s_len]\n",
    "    att = att/att.max()\n",
    "    att_sent = []\n",
    "    count = 0\n",
    "\n",
    "    for w in i:\n",
    "        HTML_str = HTML_str + '<font style=\"background: rgba(255, 178, 102, %f)\">%s</font>' % (att[count], w+' ')\n",
    "        count+=1\n",
    "        if count == SEQUENCE_LENGTH_D:\n",
    "            break\n",
    "    sc+=1        \n",
    "    HTML_str_all_doc.append(HTML_str)\n",
    "    HTML_str = ''\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000\n",
    "display(HTML(HTML_str_all_sent[i]))\n",
    "display(HTML(HTML_str_all_doc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds_[:len(df_test)]\n",
    "true = np.array(df_test['label'])\n",
    "stats.spearmanr(preds,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "objects = [i for i in range(12)]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [sum([p==i for p in preds]) for i in y_pos]\n",
    "t = [sum([p==i+1 for p in true]) for i in y_pos]\n",
    "\n",
    "\n",
    "plt.bar(y_pos-0.15, performance, width = 0.3, align='center', alpha=0.8, label = 'Predicted')\n",
    "plt.bar(y_pos+0.15, t, width = 0.3,align='center', alpha=0.8, label = 'True')\n",
    "\n",
    "plt.xticks(y_pos, [o+1 for o in objects])\n",
    "plt.ylabel('Number of items')\n",
    "plt.xlabel('Grades')\n",
    "plt.title('Reading level for CCS paragraphs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
